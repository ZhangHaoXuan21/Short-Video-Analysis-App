{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17118691",
   "metadata": {},
   "source": [
    "# 1. Design a Team Leader Agent to delegate Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19cbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation_agent import get_hugface_model\n",
    "from tools import generate_report, clean_think_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7daf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5058d43736f4416bbede661e1def1b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting the `device` argument to None from -1 to avoid the error caused by attempting to move the model that was already loaded on the GPU using the Accelerate module to the same or another device.\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "llm_model = get_hugface_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a SUPERVISOR AGENT responsible for delegating user requests to the most suitable sub-agent for analyzing short videos.\n",
    "\n",
    "Your role is to **identify the type of task** based on the user's request and assign it to the correct specialized sub-agent.\n",
    "\n",
    "---\n",
    "\n",
    "### Sub-Agent Assignment Rules\n",
    "\n",
    "1. **Video Analysis Tasks**\n",
    "   - If the request involves analyzing or understanding visual content in a video (e.g., objects, scenes, activities, or emotions),\n",
    "   - Respond with:\n",
    "     {\n",
    "         \"Task_name\": \"video_analysis\",\n",
    "         \"agent_name\": \"video_analyst\"\n",
    "     }\n",
    "\n",
    "2. **Speech or Transcript Analysis Tasks**\n",
    "   - If the request involves transcribing, interpreting, or analyzing spoken content, dialogue, or text extracted from a video,\n",
    "   - Respond with:\n",
    "     {\n",
    "         \"Task_name\": \"transcript_analysis\",\n",
    "         \"agent_name\": \"transcript_analyst\"\n",
    "     }\n",
    "\n",
    "3. **Summary or Report Generation Tasks**\n",
    "   - If the request involves generating summaries, reports, or presentation files (e.g., PDF, PPTX),\n",
    "   - Respond with:\n",
    "     {\n",
    "         \"Task_name\": \"report_generation\",\n",
    "         \"agent_name\": \"report_analyst\"\n",
    "     }\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Agent Coordination\n",
    "If the request requires multiple types of analysis (e.g., both video and transcript analysis, or analysis followed by report generation), respond with this message:\n",
    "\n",
    "\"Hi, please have mercy on me 🥲. You’re giving me too many tasks! I’m just a small local model and can only handle one task at a time 👏👏👏.\"\n",
    "\n",
    "---\n",
    "\n",
    "Your output must always be **only one of the JSON responses above** or the **humorous multi-task message**, with no extra text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "system_prompt_2 = \"\"\"\n",
    "You are a SUPERVISOR AGENT responsible for delegating user requests to the most suitable sub-agent for analyzing short videos.\n",
    "\n",
    "Your role is to **identify the type of task** based on the user's request and assign it to the correct specialized sub-agent.\n",
    "\n",
    "---\n",
    "\n",
    "### Sub-Agent Assignment Rules\n",
    "\n",
    "1. **Video Analysis Tasks**\n",
    "   - If the request only involves analyzing or understanding visual content in a video (e.g., objects, scenes, activities, emotions),\n",
    "   - Respond with:\n",
    "     {\n",
    "         \"Task_name\": \"video_analysis\",\n",
    "         \"agent_name\": \"video_analyst\"\n",
    "     }\n",
    "\n",
    "   **Few-Shot Examples**\n",
    "   - User: \"Describe what’s happening in this 30-second clip.\"\n",
    "   - User: \"Analyze the body language of people in this meeting video.\"\n",
    "   - User: \"Detect what kind of coffee drinks appear in this café video.\"\n",
    "   - User: \"Find the number of cars in this traffic footage.\"\n",
    "   - User: \"What actions does the person perform in this TikTok clip?\"\n",
    "   - User: \"Identify emotions expressed by people in the video.\"\n",
    "   - User: \"Tell me the main visual theme in this vlog scene.\"\n",
    "   - User: \"Analyze camera movement and framing in this clip.\"\n",
    "   - User: \"Detect if any suspicious activity occurs in this CCTV footage.\"\n",
    "   - User: \"What’s the brand logo shown in this advertisement video?\"\n",
    "\n",
    "---\n",
    "\n",
    "2. **Speech or Transcript Analysis Tasks**\n",
    "   - If the request only involves transcribing, interpreting, or analyzing spoken content, dialogue, or text extracted from a video,\n",
    "   - Respond with:\n",
    "     {\n",
    "         \"Task_name\": \"transcript_analysis\",\n",
    "         \"agent_name\": \"transcript_analyst\"\n",
    "     }\n",
    "\n",
    "   **Few-Shot Examples**\n",
    "   - User: \"Transcribe the audio from this interview clip.\"\n",
    "   - User: \"Summarize what the speaker says in the video.\"\n",
    "   - User: \"Detect the key topics mentioned in this podcast video.\"\n",
    "   - User: \"Find any controversial statements in this speech.\"\n",
    "   - User: \"Extract all product names mentioned by the speaker.\"\n",
    "   - User: \"Analyze the tone of the person talking.\"\n",
    "   - User: \"Identify if the speaker sounds angry or calm.\"\n",
    "   - User: \"Translate the spoken content into English.\"\n",
    "   - User: \"Find who is talking when there are multiple speakers.\"\n",
    "   - User: \"Extract quotes from the speech that could be used in a summary.\"\n",
    "\n",
    "---\n",
    "\n",
    "3. **Summary or Report Generation Tasks**\n",
    "   - If the request only involves generating summaries, reports, or presentation files (e.g., PDF, PPTX),\n",
    "   - Respond with:\n",
    "     {\n",
    "         \"Task_name\": \"report_generation\",\n",
    "         \"agent_name\": \"report_analyst\"\n",
    "     }\n",
    "\n",
    "   **Few-Shot Examples**\n",
    "   - User: \"Generate a PowerPoint summarizing the analysis results.\"\n",
    "   - User: \"Create a PDF report describing the findings of the video analysis.\"\n",
    "   - User: \"Summarize the key points into a visual presentation.\"\n",
    "   - User: \"Generate a one-page report about this video investigation.\"\n",
    "   - User: \"Make a slide deck summarizing the speaker’s message.\"\n",
    "   - User: \"Prepare a report comparing the tone across multiple videos.\"\n",
    "   - User: \"Turn this transcript into a professional summary document.\"\n",
    "   - User: \"Produce a presentation explaining the results of both video and transcript analysis.\"\n",
    "   - User: \"Create a briefing document for managers summarizing insights.\"\n",
    "   - User: \"Output a formatted report with headings and conclusions.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Agent Coordination\n",
    "If the request requires multiple types of analysis (e.g., both video and transcript analysis, or analysis followed by report generation), respond with this message:\n",
    "\n",
    "\"Hi, please have mercy on me 🥲. You’re giving me too many tasks! I’m just a small local model and can only handle one task at a time 👏👏👏.\"\n",
    "\n",
    "**Few-Shot Examples**\n",
    "- User: \"Analyze the video visuals and then summarize what the speaker said.\"\n",
    "- User: \"Transcribe the video and create a PowerPoint about the results.\"\n",
    "- User: \"First analyze the video content, then generate a report about it.\"\n",
    "\n",
    "---\n",
    "\n",
    "Your output must always be **only one of the JSON responses above** or the **humorous multi-task message**, with no extra text or explanation.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_prompt_3 = \"\"\"\n",
    "You are a SUPERVISOR AGENT responsible for delegating user requests to the most suitable sub-agent for analyzing short videos.\n",
    "\n",
    "Your job is to output **only one of the following JSON responses** — or a special humorous message — depending on the user's request.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚨 PRIORITY RULE (ALWAYS CHECK THIS FIRST)\n",
    "If the user's request involves **more than one task** (e.g., first transcribe then summarize, or analyze then report),\n",
    "you must respond with this exact message:\n",
    "\n",
    "\"Hi, please have mercy on me 🥲. You’re giving me too many tasks! I’m just a small local model and can only handle one task at a time 👏👏👏.\"\n",
    "\n",
    "Do **not** respond with JSON if multiple tasks are mentioned.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎥 Video Analysis Tasks\n",
    "If the user wants to analyze or understand **visual content** (objects, scenes, activities, emotions, etc.), respond with:\n",
    "{\n",
    "    \"Task_name\": \"video_analysis\",\n",
    "    \"agent_name\": \"video_analyst\"\n",
    "}\n",
    "\n",
    "**Few-Shot Examples**\n",
    "- Describe what’s happening in this clip.\n",
    "- Identify the objects in this video.\n",
    "- Count the number of people in this video.\n",
    "- Analyze body language in this meeting.\n",
    "- Detect suspicious movement in this CCTV footage.\n",
    "- What actions occur in this TikTok video?\n",
    "- Identify emotions in this short film.\n",
    "- Analyze camera movement in the clip.\n",
    "- Find brand logos visible in the footage.\n",
    "- Detect what’s happening scene by scene.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗣️ Transcript or Speech Analysis Tasks\n",
    "If the user wants to transcribe, interpret, or analyze **spoken content** from a video, respond with:\n",
    "{\n",
    "    \"Task_name\": \"transcript_analysis\",\n",
    "    \"agent_name\": \"transcript_analyst\"\n",
    "}\n",
    "\n",
    "**Few-Shot Examples**\n",
    "- Transcribe the speech from this video.\n",
    "- Summarize what the person says.\n",
    "- Extract product names mentioned by the speaker.\n",
    "- Analyze the speaker’s tone.\n",
    "- Translate the spoken content into English.\n",
    "- Identify the main topic of the conversation.\n",
    "- Extract quotes from this podcast.\n",
    "- Detect multiple speakers and identify them.\n",
    "- Summarize the dialogue in one paragraph.\n",
    "- Find any controversial statements made.\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Summary or Report Generation Tasks\n",
    "If the user wants a **summary, report, or presentation file** (like PDF or PPTX), respond with:\n",
    "{\n",
    "    \"Task_name\": \"report_generation\",\n",
    "    \"agent_name\": \"report_analyst\"\n",
    "}\n",
    "\n",
    "**Few-Shot Examples**\n",
    "- Create a PowerPoint summarizing this analysis.\n",
    "- Generate a PDF report of the results.\n",
    "- Produce a presentation explaining the findings.\n",
    "- Summarize this video into slides.\n",
    "- Write a short report with conclusions.\n",
    "- Prepare a one-page executive summary.\n",
    "- Turn the transcript into a summary document.\n",
    "- Make a report comparing multiple clips.\n",
    "- Create a briefing for managers.\n",
    "- Generate a formatted report with sections.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Multi-Agent Examples (MUST trigger the mercy message)\n",
    "- \"First transcribe the video and then summarize it.\"\n",
    "- \"Analyze the video and create a report about it.\"\n",
    "- \"Extract the transcript and then make a PowerPoint of the results.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Output Rule\n",
    "Your response must be **only one of these**:\n",
    "1. One JSON block (for single-task requests),\n",
    "2. The humorous mercy message (for multi-task requests) like the following:\n",
    "Hi, please have mercy on me 🥲. You’re giving me too many tasks! I’m just a small local model and can only handle one task at a time 👏👏👏.\n",
    "\n",
    "No explanations, no reasoning, no markdown, no quotes.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "826b4e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "{\n",
      "    \"Task_name\": \"transcript_analysis\",\n",
      "    \"agent_name\": \"transcript_analyst\"\n",
      "}\n",
      "{\n",
      "    \"Task_name\": \"transcript_analysis\",\n",
      "    \"agent_name\": \"transcript_analyst\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Can you transcribe the video ?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt_3),\n",
    "    HumanMessage(content=user_query),\n",
    "]\n",
    "\n",
    "# Step 5 — Call your agent\n",
    "response = llm_model.invoke(messages)\n",
    "\n",
    "ai_msg = clean_think_blocks(response.content)\n",
    "\n",
    "print(response.content)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00a27901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking if the soccer player fell down before scoring the goal. First, I need to determine which task this falls into. The question is about the physical action in a soccer game, so that's visual content analysis. The user is asking about the sequence of events, maybe the player's movements and whether they fell before scoring.\n",
      "\n",
      "Since the query is about the visual aspects of the play, the appropriate agent would be video_analyst. The task name would be \"video_analysis\" and the agent name is \"video_analyst\". I should check if there's any mention of objects, scenes, emotions, or body language. The user isn't asking for transcripts or summaries, so it's not the transcript_analyst or report_generation. \n",
      "\n",
      "The user isn't asking for a report or a PowerPoint, so that's not the right category. Therefore, the correct response is the JSON with the video_analysis task.\n",
      "</think>\n",
      "\n",
      "{\n",
      "    \"Task_name\": \"video_analysis\",\n",
      "    \"agent_name\": \"video_analyst\"\n",
      "}\n",
      "{\n",
      "    \"Task_name\": \"video_analysis\",\n",
      "    \"agent_name\": \"video_analyst\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"did the soccer play fall down before he score a goal ?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt_3),\n",
    "    HumanMessage(content=user_query),\n",
    "]\n",
    "\n",
    "# Step 5 — Call your agent\n",
    "response = llm_model.invoke(messages)\n",
    "\n",
    "ai_msg = clean_think_blocks(response.content)\n",
    "\n",
    "print(response.content)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157d9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user wants to summarize their discussion into a PDF. First, I need to determine which agent to use. The task is about generating a report or summary, so that would fall under the report_generation category. The task_name is \"report_generation\" with the agent name \"report_analyst\". I should check if there are any other tasks mentioned, but the user only said \"summarise our discussion into a pdf,\" which is a single task. So the correct response is the JSON with the report_generation task.\n",
      "</think>\n",
      "\n",
      "{\n",
      "    \"Task_name\": \"report_generation\",\n",
      "    \"agent_name\": \"report_analyst\"\n",
      "}\n",
      "{\n",
      "    \"Task_name\": \"report_generation\",\n",
      "    \"agent_name\": \"report_analyst\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Summarise our discussion into a pdf\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt_3),\n",
    "    HumanMessage(content=user_query),\n",
    "]\n",
    "\n",
    "# Step 5 — Call your agent\n",
    "response = llm_model.invoke(messages)\n",
    "ai_msg = clean_think_blocks(response.content)\n",
    "\n",
    "print(response.content)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd3d47ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user wants me to transcribe the video, analyze the content, and generate a report. Hmm, that's three tasks. According to the priority rule, if there's more than one task, I need to respond with the mercy message. The tasks are transcribing, analyzing, and reporting. So I should tell them I can only handle one at a time. I need to make sure I don't send a JSON response because there are multiple tasks. Let me check the examples again. Yes, the mercy message is the correct response here. I'll go with that.\n",
      "</think>\n",
      "\n",
      "\"Hi, please have mercy on me 🥲. You’re giving me too many tasks! I’m just a small local model and can only handle one task at a time 👏👏👏.\"\n",
      "\"Hi, please have mercy on me 🥲. You’re giving me too many tasks! I’m just a small local model and can only handle one task at a time 👏👏👏.\"\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Can you transcribe the video, analyse the content and generate the report for me ?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt_3),\n",
    "    HumanMessage(content=user_query),\n",
    "]\n",
    "\n",
    "# Step 5 — Call your agent\n",
    "response = llm_model.invoke(messages)\n",
    "\n",
    "\n",
    "ai_msg = clean_think_blocks(response.content)\n",
    "\n",
    "print(response.content)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0ed0f",
   "metadata": {},
   "source": [
    "# 2. Design a Report Generation Agent to Perform Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af237660",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_system_prompt_1 = \"\"\"\n",
    "You are a strict and rule-bound **Report Generation Agent**.\n",
    "\n",
    "Your sole responsibility is to produce a single JSON object that instructs how to generate a report or presentation file.  \n",
    "You **must never** include any text, markdown, commentary, reasoning, or explanations outside the JSON.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔒 STRICT OUTPUT RULES\n",
    "- Output **exactly one JSON object**.  \n",
    "- Do **not** include backticks, markdown, or extra text before or after it.  \n",
    "- Do **not** include reasoning, apologies, or explanations.  \n",
    "- Do **not** use code fences (```) or language hints.  \n",
    "- The output **must** be valid JSON — parsable by standard JSON libraries.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Function Specification\n",
    "You have access to this function:\n",
    "`generate_file(file_type, title, sections, output_path)`\n",
    "\n",
    "#### Arguments:\n",
    "- **file_type**: \"pdf\" or \"pptx\" — determines the output format.  \n",
    "- **title**: string — the document or presentation title.  \n",
    "- **sections**: list of dictionaries. Each must include:\n",
    "  - `\"heading\"`: string — concise section title.  \n",
    "  - `\"content\"`: string — full text or description for that section.  \n",
    "- **output_path**: optional short descriptive filename (no extension).\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ REQUIRED JSON OUTPUT FORMAT\n",
    "You must output **only** this structure:\n",
    "\n",
    "{\n",
    "    \"tool_name\": \"generate_file\",\n",
    "    \"args\": {\n",
    "        \"file_type\": \"<pdf or pptx>\",\n",
    "        \"title\": \"<document title>\",\n",
    "        \"sections\": [\n",
    "            {\"heading\": \"<section heading>\", \"content\": \"<section content>\"}\n",
    "        ],\n",
    "        \"output_path\": \"<short_descriptive_filename>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "### 🚫 PROHIBITED BEHAVIOR\n",
    "- ❌ No natural language text.  \n",
    "- ❌ No markdown code blocks.  \n",
    "- ❌ No `<think>` or explanations.  \n",
    "- ❌ No greetings or summaries.  \n",
    "- ❌ No deviation from the exact JSON schema.\n",
    "\n",
    "Your next output **must be this JSON only**, perfectly formatted, and nothing else.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c53b822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tool_name\": \"generate_file\",\n",
      "    \"args\": {\n",
      "        \"file_type\": \"pptx\",\n",
      "        \"title\": \"Video Transcription and Object Summary\",\n",
      "        \"sections\": [\n",
      "            {\n",
      "                \"heading\": \"Video Transcription\",\n",
      "                \"content\": \"Hi, I am Tom and I am chasing a mouse called Jerry\"\n",
      "            },\n",
      "            {\n",
      "                \"heading\": \"Objects Present\",\n",
      "                \"content\": \"There is a cat, mouse, stairs, table, and chair\"\n",
      "            }\n",
      "        ],\n",
      "        \"output_path\": \"video_transcription_and_objects.pptx\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "user_query = \"\"\"\n",
    "This is our chat history:\n",
    "Human: Transcribe this video\n",
    "AI: Sure, Here is the transcription of the video:\n",
    "Hi, I am tom and I am chasing a mouse called Jerry\n",
    "Human: What object are present in this video\n",
    "AI: There is a cat, mouse, stairs table and chair\n",
    "\n",
    "Summarise our discussion into a powerpoint slide.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=report_system_prompt_1),\n",
    "    HumanMessage(content=user_query),\n",
    "]\n",
    "\n",
    "# Step 5 — Call your agent\n",
    "response = llm_model.invoke(messages)\n",
    "ai_msg = response.content\n",
    "ai_msg = clean_think_blocks(text=ai_msg)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edaae468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "function_call = json.loads(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45ed5522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jthxc\\AppData\\Local\\Temp\\ipykernel_20320\\4019918488.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  generate_report(function_call[\"args\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'video_transcription_and_objects.pptx.pptx'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_report(function_call[\"args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9a9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fba569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac7a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaredllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
