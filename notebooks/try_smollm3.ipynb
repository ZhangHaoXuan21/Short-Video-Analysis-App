{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c440532a",
   "metadata": {},
   "source": [
    "# 1. Try SmolLM2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef38325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def get_smollm2_model() -> ChatHuggingFace:\n",
    "    # === 1. Define model and directory ===\n",
    "\n",
    "    # Qwen/Qwen3-4B-Thinking-2507,  Qwen/Qwen3-4B-Instruct-2507 , HuggingFaceTB/SmolLM2-1.7B-Instruct , Qwen/Qwen3-1.7B, HuggingFaceTB/SmolLM3-3B\n",
    "    model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "    #model_dir = Path(__file__).parent.parent / \"model\" / \"HuggingFaceTB/SmolLM3-3B\"\n",
    "    save_dir = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\model\\SmolLM2-1.7B-Instruct\"\n",
    "\n",
    "    # === 2. Configure 4-bit quantization ===\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=model_name,\n",
    "        task=\"text-generation\",\n",
    "        pipeline_kwargs=dict(\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.03,\n",
    "            return_full_text=False,\n",
    "            \n",
    "        ),\n",
    "        model_kwargs={\n",
    "            \"cache_dir\": save_dir,\n",
    "            \"quantization_config\": bnb_config,\n",
    "            \"device_map\": \"auto\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "    return chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c75564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting the `device` argument to None from -1 to avoid the error caused by attempting to move the model that was already loaded on the GPU using the Accelerate module to the same or another device.\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "model = get_supervisor_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18739857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"what is llm in short ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee163f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Language Model. It refers to a type of artificial intelligence that can understand and generate human language. It's designed to mimic the way humans communicate, making it useful for tasks like language translation, text summarization, and chatbot interactions.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bab321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript_analyst\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        \"You are a Team Leader collaborating with two agents:\\n\"\n",
    "        \"- Video Agent: provides visual analysis\\n\"\n",
    "        \"- Transcript Agent: provides text transcription\\n\\n\"\n",
    "        \"Use their responses to answer user queries.\\n\"\n",
    "        \"If you need video analysis, reply ONLY with: video_analyst\\n\"\n",
    "        \"If you need video transcription, reply ONLY with: transcript_analyst\"\n",
    "    )),\n",
    "    HumanMessage(content=(\n",
    "        \"Can you transcribe this video?\"\n",
    "    ))\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaredllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
