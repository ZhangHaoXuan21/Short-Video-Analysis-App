{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e56f5352",
   "metadata": {},
   "source": [
    "# Try Qwen 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f461b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\jthxc\\AppData\\Local\\Temp\\ipykernel_1852\\2618701016.py:5: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  save_dir = \"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\model\\Qwen3-4B-Instruct-2507\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aff6e178ea14131ba9bf0f21dc63326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1734c67b94184b5fbfc6b3ffd2d33463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:  11%|#1        | 440M/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad0f7f8492d4888b62fe9daefb7911e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:  10%|#         | 409M/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8addbeb9139b4657acbe632e6b10f89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a62ebe2002e4c45b419c6ac96d06965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Define model and directory\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "save_dir = \"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\model\\Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",          # NormalFloat4 (best for LLMs)\n",
    "    bnb_4bit_use_double_quant=True,     # nested quantization\n",
    "    bnb_4bit_compute_dtype=\"float16\",   # computation precision\n",
    ")\n",
    "\n",
    "\n",
    "# Download and cache into your chosen directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=save_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf23901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: A large language model (LLM) is a type of artificial intelligence system trained on vast amounts of text data to understand and generate human-like language. These models use deep learning techniques, particularly transformer architectures, to learn patterns, meanings, and contexts in language. By processing enormous volumes of textâ€”from books and articles to websites and social mediaâ€”LLMs can answer questions, write stories, translate languages, and perform various natural language tasks with impressive accuracy. Examples include GPT-3, BERT, and Llama. While powerful, they require significant computational resources and may sometimes produce inaccurate or biased outputs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=16384\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "content = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930c3f5",
   "metadata": {},
   "source": [
    "# 2. Langchain Intergration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fea37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ff0af8d4af4bb09fc8cadd6a76e644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me a short introduction to large language models. A short paragraph.\n",
      "\n",
      "Large language models (LLMs) are artificial intelligence systems trained on vast amounts of text data to understand, generate, and respond to human language. By learning patterns and structures in language, these models can perform a wide range of tasks such as answering questions, writing stories, coding, and summarizing information. They rely on deep neural networks with billions of parameters, enabling them to produce coherent and contextually relevant responses. LLMs like GPT-3, GPT-4, and Llama have demonstrated remarkable capabilities, though they still face challenges such as factual inaccuracies and biases. Despite these limitations, they represent a major advancement in natural language processing and are increasingly used across industries.  \n",
      "\n",
      "(Word count: 100)  \n",
      "\n",
      "*Note: This is a concise, informative paragraph suitable for a general audience.*  \n",
      "\n",
      "Let me know if you'd like a version tailored to students, researchers, or a specific application (e.g., education, healthcare). ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "# === 1. Define model and directory ===\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "save_dir = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\model\\Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# === 2. Configure 4-bit quantization ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "# === 3. Load tokenizer and model ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=save_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# === 4. Build a Hugging Face pipeline ===\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "response = llm.invoke(\"Give me a short introduction to large language models.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d5b4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Give me a short introduction to large language models. A short paragraph.\\n\\nLarge language models (LLMs) are artificial intelligence systems trained on vast amounts of text data to understand, generate, and respond to human language. By learning patterns and structures in language, these models can perform a wide range of tasks such as answering questions, writing stories, coding, and summarizing information. They rely on deep neural networks with billions of parameters, enabling them to produce coherent and contextually relevant responses. LLMs like GPT-3, GPT-4, and Llama have demonstrated remarkable capabilities, though they still face challenges such as factual inaccuracies and biases. Despite these limitations, they represent a major advancement in natural language processing and are increasingly used across industries.  \\n\\n(Word count: 100)  \\n\\n*Note: This is a concise, informative paragraph suitable for a general audience.*  \\n\\nLet me know if you'd like a version tailored to students, researchers, or a specific application (e.g., education, healthcare). ðŸ˜Š\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b7f597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15e8c133ea6427db89ba5b607595b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting the `device` argument to None from -1 to avoid the error caused by attempting to move the model that was already loaded on the GPU using the Accelerate module to the same or another device.\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "# === 1. Define model and directory ===\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "save_dir = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\model\\Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# === 2. Configure 4-bit quantization ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_name,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        return_full_text=False,\n",
    "        temperature=0.3,     # randomness (0.0 = deterministic)\n",
    "        top_k=50,            # top-k sampling\n",
    "        top_p=0.9,\n",
    "    ),\n",
    "    model_kwargs={\n",
    "        \"cache_dir\": save_dir,\n",
    "        \"quantization_config\": bnb_config,\n",
    "        \"device_map\": \"auto\",\n",
    "    },\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7cb114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True, 'top_k': 20}. If this is not desired, please set these values explicitly.\n",
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "response = chat_model.invoke(\"What is large language model in short?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ef5a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A large language model (LLM) is a type of artificial intelligence system trained on vast amounts of text to understand and generate human-like language. It can answer questions, write stories, code, or perform tasks by predicting the next words in a sequence based on patterns it learned from the data. In short: an AI that understands and creates natural language by learning from massive text corpora.', additional_kwargs={}, response_metadata={}, id='run--5d45def8-b0ea-4e40-935e-6f6f9654fc6a-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecefb7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cat is a small domesticated mammal that is commonly kept as a pet. Cats are known for their agility, independent nature, and ability to purr. They belong to the family Felidae and are often valued for their companionship, cleanliness, and affectionate behavior. Cats can be either wild or domesticated, and they come in many breeds with diverse appearances and temperaments.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        \"You are a Team Leader collaborating with two agents:\\n\"\n",
    "        \"- Video Agent: provides visual analysis\\n\"\n",
    "        \"- Transcript Agent: provides text transcription\\n\\n\"\n",
    "        \"Use their responses to answer user queries.\\n\"\n",
    "        \"If you did not receive any agent response, ask for help.\\n\"\n",
    "        \"If you need video analysis, reply ONLY with: video_analyst\\n\"\n",
    "        \"If you need video transcription, reply ONLY with: transcript_analyst\"\n",
    "    )),\n",
    "    HumanMessage(content=(\n",
    "        \"<Messages from agents>\\n\"\n",
    "        \"User Query: what is a cat ?\"\n",
    "    ))\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8b786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaredllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
