{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal\n",
    "# import torch\n",
    "# from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "# from pathlib import Path\n",
    "\n",
    "\n",
    "# class SmolVLM2ChatModel:\n",
    "#     \"\"\"\n",
    "#     LangChain-style wrapper for HuggingFace SmolVLM2 that supports chat-like .invoke() calls.\n",
    "#     Automatically selects between 256M, 500M, and 2.2B model variants based on GPU memory.\n",
    "#     Supports quantization using bitsandbytes (4-bit or 8-bit).\n",
    "#     \"\"\"\n",
    "\n",
    "#     MODEL_MAP = {\n",
    "#         \"small\": \"HuggingFaceTB/SmolVLM2-256M-Instruct\",\n",
    "#         \"medium\": \"HuggingFaceTB/SmolVLM2-500M-Instruct\",\n",
    "#         \"large\": \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n",
    "#     }\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_size: Literal[\"small\", \"medium\", \"large\"] | None = None,\n",
    "#         device: str | None = None,\n",
    "#         dtype: torch.dtype = torch.bfloat16,\n",
    "#         quantization: Literal[\"none\", \"8bit\", \"4bit\"] = \"none\",\n",
    "#         model_root: str | Path | None = None,  # üëà optionally specify local model root\n",
    "#     ):\n",
    "#         self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         self.quantization = quantization\n",
    "\n",
    "#         # === 1. Auto-select model size if not given ===\n",
    "#         if model_size is None:\n",
    "#             model_size = self._auto_select_model_size()\n",
    "\n",
    "#         self.model_name = self.MODEL_MAP.get(model_size, self.MODEL_MAP[\"medium\"])\n",
    "#         print(f\"üîπ Preparing to load {self.model_name} ({model_size}) with {quantization} quantization...\")\n",
    "\n",
    "#         # === 2. Define local cache directory ===\n",
    "#         model_root = Path(model_root) if model_root else Path(__file__).parent.parent / \"model\"\n",
    "#         self.model_dir = model_root / self.model_name.split(\"/\")[-1]\n",
    "#         self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "#         print(f\"üìÇ Model cache directory: {self.model_dir}\")\n",
    "\n",
    "#         # === 3. Configure quantization ===\n",
    "#         quant_config = None\n",
    "#         if quantization in [\"8bit\", \"4bit\"]:\n",
    "#             try:\n",
    "#                 quant_config = BitsAndBytesConfig(\n",
    "#                     load_in_8bit=(quantization == \"8bit\"),\n",
    "#                     load_in_4bit=(quantization == \"4bit\"),\n",
    "#                     bnb_4bit_quant_type=\"nf4\",\n",
    "#                     bnb_4bit_use_double_quant=True,\n",
    "#                     bnb_4bit_compute_dtype=torch.float16,\n",
    "#                 )\n",
    "#                 print(f\"‚öôÔ∏è Using bitsandbytes {quantization} quantization.\")\n",
    "#             except Exception as e:\n",
    "#                 print(\"‚ö†Ô∏è Quantization unavailable, proceeding without it:\", e)\n",
    "\n",
    "#         # === 4. Load processor ===\n",
    "#         self.processor = AutoProcessor.from_pretrained(\n",
    "#             self.model_name,\n",
    "#             cache_dir=self.model_dir\n",
    "#         )\n",
    "\n",
    "#         # === 5. Load model ===\n",
    "#         print(f\"üöÄ Loading model weights into {self.device}...\")\n",
    "#         self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "#             self.model_name,\n",
    "#             cache_dir=self.model_dir,\n",
    "#             torch_dtype=dtype,\n",
    "#             quantization_config=quant_config,\n",
    "#             device_map=\"auto\" if quant_config else None,\n",
    "#         )\n",
    "\n",
    "#         # === 6. Move to device if not quantized ===\n",
    "#         if not quant_config:\n",
    "#             self.model.to(self.device)\n",
    "\n",
    "#         print(f\"‚úÖ Successfully loaded {model_size.upper()} SmolVLM2 model ({self.device})!\")\n",
    "\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     def _auto_select_model_size(self):\n",
    "#         \"\"\"Automatically select model size based on available GPU VRAM.\"\"\"\n",
    "#         if not torch.cuda.is_available():\n",
    "#             print(\"‚ö†Ô∏è CUDA not available. Using CPU-friendly 256M model.\")\n",
    "#             return \"small\"\n",
    "\n",
    "#         try:\n",
    "#             total_vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "#             print(f\"üßÆ Detected VRAM: {total_vram_gb:.1f} GB\")\n",
    "\n",
    "#             if total_vram_gb < 6:\n",
    "#                 return \"small\"\n",
    "#             elif total_vram_gb < 10:\n",
    "#                 return \"medium\"\n",
    "#             else:\n",
    "#                 return \"large\"\n",
    "#         except Exception as e:\n",
    "#             print(\"‚ö†Ô∏è Could not detect VRAM:\", e)\n",
    "#             return \"medium\"\n",
    "\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     def _format_messages(self, messages):\n",
    "#         \"\"\"Format messages for processor chat template.\"\"\"\n",
    "#         if not isinstance(messages, list):\n",
    "#             raise ValueError(\"messages must be a list of message dicts.\")\n",
    "#         return messages\n",
    "\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     def _generate(self, formatted_messages, max_new_tokens=512, do_sample=False):\n",
    "#         inputs = self.processor.apply_chat_template(\n",
    "#             formatted_messages,\n",
    "#             add_generation_prompt=True,\n",
    "#             tokenize=True,\n",
    "#             return_dict=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).to(self.device, dtype=torch.bfloat16)\n",
    "\n",
    "#         outputs = self.model.generate(\n",
    "#             **inputs,\n",
    "#             do_sample=do_sample,\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#         )\n",
    "\n",
    "#         return self.processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     def invoke(self, input):\n",
    "#         \"\"\"LangChain-style .invoke() method for single chat turn.\"\"\"\n",
    "#         formatted_messages = self._format_messages([input])\n",
    "#         return {\"type\": \"ai_message\", \"content\": self._generate(formatted_messages)}\n",
    "\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     def batch(self, inputs):\n",
    "#         \"\"\"LangChain-style .batch() method for multiple chat turns.\"\"\"\n",
    "#         results = []\n",
    "#         for inp in inputs:\n",
    "#             formatted_messages = self._format_messages([inp])\n",
    "#             response = self._generate(formatted_messages)\n",
    "#             results.append({\"type\": \"ai_message\", \"content\": response})\n",
    "#         return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f2048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Preparing to load HuggingFaceTB/SmolVLM2-500M-Instruct (medium) with 4bit quantization...\n",
      "üìÇ Model cache directory: c:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\model\\SmolVLM2-500M-Instruct\n",
      "‚öôÔ∏è Using bitsandbytes 4bit quantization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b180ed10f0f042e491f4a48eab5ebc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/67.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\model\\SmolVLM2-500M-Instruct\\models--HuggingFaceTB--SmolVLM2-500M-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827a6779063f492fb2b3972faa650ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3289b9eb5bed44188ffc76cd97507481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0315076b718b499399e7888e1afd3c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c060ec25f3a7449a97eda6569c78ad28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f591f3dbbad4873b4b09e9cd4014218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccc9b93de644c8fb4bbdca395bd8e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4b4e18130e4e50bca27198df8b3c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94abe17233d94722ab6a2ce75b1725cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/868 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading model weights into cuda...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d87692119ee4dec9802a1f0df82ecc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367d30cd19d1451cb1050dd809876901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb561da24f44e77abb66cf47334807a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded MEDIUM SmolVLM2 model (cuda)!\n"
     ]
    }
   ],
   "source": [
    "from video_agent import SmolVLM2ChatModel\n",
    "from tools import extract_assistant_response\n",
    "\n",
    "chat_model = SmolVLM2ChatModel(quantization=\"4bit\", model_size=\"medium\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_assistant_response(text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Extracts the Assistant's actual response text from a raw SmolVLM output.\n",
    "\n",
    "#     Example:\n",
    "#         Input:\n",
    "#             \"üß† Response: User: You are provided the following... Assistant: Yes, there is a graph...\"\n",
    "#         Output:\n",
    "#             \"Yes, there is a graph in the video. It shows the progress...\"\n",
    "#     \"\"\"\n",
    "#     # Remove leading emojis or response prefixes like \"üß† Response:\"\n",
    "#     cleaned_text = re.sub(r\"^üß†\\s*Response:\\s*\", \"\", text.strip())\n",
    "\n",
    "#     # Use regex to find text after 'Assistant:'\n",
    "#     match = re.search(r\"Assistant:\\s*(.*)\", cleaned_text, re.DOTALL)\n",
    "#     if match:\n",
    "#         # Clean trailing whitespace/newlines\n",
    "#         return match.group(1).strip()\n",
    "#     return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89911d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\transformers\\video_processing_utils.py:879: UserWarning: `torchcodec` is not installed and cannot be used to decode the video by default. Falling back to `torchvision`. Note that `torchvision` decoding is deprecated and will be removed in future versions. \n",
      "  warnings.warn(\n",
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\transformers\\video_utils.py:524: UserWarning: Using `torchvision` for video decoding is deprecated and will be removed in future versions. Please use `torchcodec` instead.\n",
      "  warnings.warn(\n",
      "You have used fast image processor with LANCZOS resample which not yet supported for torch.Tensor. BICUBIC resample will be used as an alternative. Please fall back to image processor if you want full consistency with the original model.\n",
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video captures a soccer match between FC Barcelona and Atletico Madrid, featuring a player in a white and gold uniform celebrating a goal. The scene is set in a stadium filled with spectators, with a Coca-Cola advertisement visible in the background. The player in the white and gold uniform is seen celebrating the goal, with his arms raised in the air, and his teammates nearby. The video then shifts to a different scene, showing a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then transitions to a different player in a white and gold uniform, who is seen celebrating a goal, with a crowd of spectators in the background. The video continues with a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The video continues with a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The video continues with a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The video continues with a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold uniform celebrating a goal, with a crowd of spectators in the background. The scene then shifts to a player in a white and gold\n"
     ]
    }
   ],
   "source": [
    "# Example 1: single invoke\n",
    "#video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\What is MCP and how does it change AI_ (MCP explained) #ai #artificialintelligence (1).mp4\"\n",
    "\n",
    "#video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\cat.mp4\"\n",
    "\n",
    "video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\Son strikes first üöÄ.mp4\"\n",
    "\n",
    "#video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\Robbing a bank in 2028.mp4\"\n",
    "\n",
    "#video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\n8n autoamtion.mp4\"\n",
    "\n",
    "user_query = \"Describe what you see in the video\"\n",
    "\n",
    "response = chat_model.invoke({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"video\", \"path\": video_path},\n",
    "        {\"type\": \"text\", \"text\": user_query}\n",
    "    ]\n",
    "})\n",
    "\n",
    "ai_msg = extract_assistant_response(response[\"content\"])\n",
    "\n",
    "print(f\"{ai_msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b255b895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"User: You are provided the following series of twenty-three frames from a 0:00:23 [H:MM:SS] video.\\n\\nFrame from 00:00:\\nFrame from 00:01:\\nFrame from 00:02:\\nFrame from 00:03:\\nFrame from 00:04:\\nFrame from 00:05:\\nFrame from 00:06:\\nFrame from 00:07:\\nFrame from 00:08:\\nFrame from 00:09:\\nFrame from 00:10:\\nFrame from 00:11:\\nFrame from 00:12:\\nFrame from 00:13:\\nFrame from 00:14:\\nFrame from 00:15:\\nFrame from 00:16:\\nFrame from 00:18:\\nFrame from 00:19:\\nFrame from 00:20:\\nFrame from 00:21:\\nFrame from 00:22:\\nFrame from 00:23:\\n\\nCan you tell me the objects that are present in the video ?\\nAssistant: The video features a white and brown cat with a red bandana around its neck. The cat is sitting on a tiled floor, which is part of a kitchen area. The cat is positioned in the center of the frame, and it appears to be in a playful or curious mood. The cat is looking towards the camera, which suggests that it is interested in what the camera is capturing. The cat's position and the angle of the camera suggest that the video was likely taken from a low angle, which makes the cat appear larger and more imposing. The cat's position and the angle of the\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b60f0",
   "metadata": {},
   "source": [
    "# 2. Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b51901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "import re\n",
    "\n",
    "class SmolVLM2ChatModel:\n",
    "    \"\"\"\n",
    "    LangChain-style wrapper for HuggingFace SmolVLM2 that supports:\n",
    "      ‚úÖ chat-like .invoke() calls\n",
    "      ‚úÖ automatic model size selection\n",
    "      ‚úÖ quantization (4-bit / 8-bit)\n",
    "      ‚úÖ video analysis of up to ~1 minute (frame sampling + summarization)\n",
    "    \"\"\"\n",
    "\n",
    "    MODEL_MAP = {\n",
    "        \"small\": \"HuggingFaceTB/SmolVLM2-256M-Instruct\",\n",
    "        \"medium\": \"HuggingFaceTB/SmolVLM2-500M-Instruct\",\n",
    "        \"large\": \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_size: Literal[\"small\", \"medium\", \"large\"] | None = None,\n",
    "        device: str | None = None,\n",
    "        dtype: torch.dtype = torch.bfloat16,\n",
    "        quantization: Literal[\"none\", \"8bit\", \"4bit\"] = \"none\",\n",
    "    ):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.quantization = quantization\n",
    "\n",
    "        # Auto-select model size if not specified\n",
    "        if model_size is None:\n",
    "            model_size = self._auto_select_model_size()\n",
    "\n",
    "        self.model_name = self.MODEL_MAP.get(model_size, self.MODEL_MAP[\"medium\"])\n",
    "        print(f\"üîπ Loading {self.model_name} on {self.device} with {quantization} quantization...\")\n",
    "\n",
    "        # Configure quantization if requested\n",
    "        quant_config = None\n",
    "        if quantization in [\"8bit\", \"4bit\"]:\n",
    "            try:\n",
    "                quant_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=(quantization == \"8bit\"),\n",
    "                    load_in_4bit=(quantization == \"4bit\"),\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_compute_dtype=dtype,\n",
    "                )\n",
    "                print(f\"‚öôÔ∏è Using {quantization} quantization with bitsandbytes.\")\n",
    "            except Exception as e:\n",
    "                print(\"‚ö†Ô∏è Quantization not available, loading normally:\", e)\n",
    "\n",
    "        # Load processor and model\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\" if quant_config else None,\n",
    "            quantization_config=quant_config,\n",
    "        ).to(self.device if not quant_config else None)\n",
    "\n",
    "        print(f\"‚úÖ {model_size.upper()} SmolVLM2 model loaded successfully!\\n\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _auto_select_model_size(self):\n",
    "        \"\"\"Automatically select model size based on available GPU VRAM.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"‚ö†Ô∏è CUDA not available. Using CPU-friendly 256M model.\")\n",
    "            return \"small\"\n",
    "\n",
    "        try:\n",
    "            total_vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"üßÆ Detected VRAM: {total_vram_gb:.1f} GB\")\n",
    "\n",
    "            if total_vram_gb < 6:\n",
    "                return \"small\"\n",
    "            elif total_vram_gb < 10:\n",
    "                return \"medium\"\n",
    "            else:\n",
    "                return \"large\"\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Could not detect VRAM:\", e)\n",
    "            return \"medium\"\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _format_messages(self, messages):\n",
    "        \"\"\"Format messages for chat template.\"\"\"\n",
    "        if not isinstance(messages, list):\n",
    "            raise ValueError(\"messages must be a list of message dicts.\")\n",
    "        return messages\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _generate(self, formatted_messages, max_new_tokens=512, do_sample=False):\n",
    "        inputs = self.processor.apply_chat_template(\n",
    "            formatted_messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device, dtype=torch.bfloat16)\n",
    "\n",
    "        outputs = self.model.generate(**inputs, do_sample=do_sample, max_new_tokens=max_new_tokens)\n",
    "        return self.processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def invoke(self, input):\n",
    "        \"\"\"LangChain-style .invoke() method for single chat turn.\"\"\"\n",
    "        formatted_messages = self._format_messages([input])\n",
    "        return {\"type\": \"ai_message\", \"content\": self._generate(formatted_messages)}\n",
    "\n",
    "    def batch(self, inputs):\n",
    "        \"\"\"LangChain-style .batch() method for multiple chat turns.\"\"\"\n",
    "        results = []\n",
    "        for inp in inputs:\n",
    "            formatted_messages = self._format_messages([inp])\n",
    "            response = self._generate(formatted_messages)\n",
    "            results.append({\"type\": \"ai_message\", \"content\": response})\n",
    "        return results\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # üé• --- Video Analysis Extension ---\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    def _sample_video_frames(self, video_path: str, num_frames: int = 12):\n",
    "        \"\"\"Sample evenly spaced frames from the video.\"\"\"\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "        output_dir = Path(\"sampled_frames\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        frame_paths = []\n",
    "        for i in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_path = output_dir / f\"frame_{i}.jpg\"\n",
    "                cv2.imwrite(str(frame_path), frame)\n",
    "                frame_paths.append(str(frame_path))\n",
    "        cap.release()\n",
    "        return frame_paths\n",
    "\n",
    "    def _analyze_frame(self, frame_path: str, question: str, idx: int, total: int):\n",
    "        \"\"\"Analyze a single frame using SmolVLM2.\"\"\"\n",
    "        image = Image.open(frame_path)\n",
    "        # ‚úÖ Add <image> token so SmolVLM2 knows an image is provided\n",
    "        prompt = f\"<image>\\nFrame {idx+1}/{total}: {question}\"\n",
    "        inputs = self.processor(text=prompt, images=[image], return_tensors=\"pt\").to(self.device)\n",
    "        output = self.model.generate(**inputs, max_new_tokens=120)\n",
    "        return self.processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    " \n",
    "\n",
    "    def analyze_video(self, video_path: str, question: str, num_frames: int = 12):\n",
    "        \"\"\"Analyze a video and return cleaned, non-empty frame results while preserving the user query.\"\"\"\n",
    "        print(f\"üé¨ Sampling {num_frames} frames from {video_path}...\")\n",
    "        frame_paths = self._sample_video_frames(video_path, num_frames)\n",
    "        print(f\"‚úÖ {len(frame_paths)} frames extracted.\\n\")\n",
    "\n",
    "        frame_results = []\n",
    "\n",
    "        def clean_text(text: str) -> str:\n",
    "            \"\"\"Clean model output but keep the original user question.\"\"\"\n",
    "            text = text.strip()\n",
    "\n",
    "            # Remove the \"Frame X/Y:\" prefix only\n",
    "            text = re.sub(r\"^Frame\\s*\\d+/\\d+:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Remove empty lines and redundant spaces\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "            # Remove consecutive duplicate sentences\n",
    "            sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "            seen = set()\n",
    "            cleaned_sentences = []\n",
    "            for s in sentences:\n",
    "                s_clean = s.strip()\n",
    "                if s_clean and s_clean.lower() not in seen:\n",
    "                    cleaned_sentences.append(s_clean)\n",
    "                    seen.add(s_clean.lower())\n",
    "            text = \" \".join(cleaned_sentences)\n",
    "\n",
    "            # If text only repeats the user query (no description), skip it later\n",
    "            return text.strip()\n",
    "\n",
    "        for idx, frame_path in enumerate(frame_paths):\n",
    "            print(f\"üñºÔ∏è Analyzing frame {idx+1}/{len(frame_paths)}...\")\n",
    "            try:\n",
    "                result = self._analyze_frame(frame_path, question, idx, len(frame_paths))\n",
    "                cleaned = clean_text(result)\n",
    "\n",
    "                # Skip empty or echo-only results\n",
    "                if cleaned and cleaned.lower() != question.lower():\n",
    "                    print(f\"üìù Frame {idx+1} cleaned result:\", cleaned, \"\\n\")\n",
    "                    frame_results.append({\n",
    "                        \"frame_index\": idx,\n",
    "                        \"frame_path\": frame_path,\n",
    "                        \"analysis\": cleaned\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Frame {idx+1} produced no new information, skipping.\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error analyzing frame {idx+1}: {e}\")\n",
    "\n",
    "        print(f\"‚úÖ Video analysis complete. {len(frame_results)} valid, cleaned analyses collected.\")\n",
    "        return {\n",
    "            \"frames_analyzed\": len(frame_results),\n",
    "            \"results\": frame_results\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8bd9aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading HuggingFaceTB/SmolVLM2-500M-Instruct on cuda with 4bit quantization...\n",
      "‚öôÔ∏è Using 4bit quantization with bitsandbytes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MEDIUM SmolVLM2 model loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SmolVLM2ChatModel(model_size=\"medium\", quantization=\"4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b112915e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Sampling 16 frames from C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\cat.mp4...\n",
      "‚úÖ 16 frames extracted.\n",
      "\n",
      "üñºÔ∏è Analyzing frame 1/16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Frame 1 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat is wearing a red and white bandana. The cat is chewing on its paw. The cat is sitting on the floor. The cat is looking at the camera. \n",
      "\n",
      "üñºÔ∏è Analyzing frame 2/16...\n",
      "üìù Frame 2 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat is wearing a red bandana. The cat is sitting on the floor. The cat is looking at the camera. The cat \n",
      "\n",
      "üñºÔ∏è Analyzing frame 3/16...\n",
      "üìù Frame 3 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat is wearing a red collar. The background is a kitchen with a refrigerator. The cat is sitting on the floor. The cat is looking at the camera. The cat \n",
      "\n",
      "üñºÔ∏è Analyzing frame 4/16...\n",
      "üìù Frame 4 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking up at the camera. The cat is wearing a red bow tie. The cat is sitting on the floor. The cat is looking up at the camera. The cat is looking \n",
      "\n",
      "üñºÔ∏è Analyzing frame 5/16...\n",
      "üìù Frame 5 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, wearing a red bandana. The cat is looking at the camera. The cat is sitting on the floor. The cat is wearing a red bandana. The cat is sitting on \n",
      "\n",
      "üñºÔ∏è Analyzing frame 6/16...\n",
      "üìù Frame 6 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor. The cat is wearing a red and white bow. The cat is looking at the camera. The cat is wearing a red and white \n",
      "\n",
      "üñºÔ∏è Analyzing frame 7/16...\n",
      "üìù Frame 7 cleaned result: Describe the main actions, objects, and sequence of events. The cat is standing on its hind legs, looking up at the camera. The cat has a red collar on its neck. The background is a kitchen with a refrigerator and a stove. The cat is looking at the camera. The cat is not moving. The cat is not eating or drinking. The cat is not playing with the camera. The cat is not wearing a hat. The cat is not \n",
      "\n",
      "üñºÔ∏è Analyzing frame 8/16...\n",
      "üìù Frame 8 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking up at the camera. The cat is wearing a red bow tie. The background is a room with a beige wall and a black door. The cat is sitting on the floor. The cat is looking up at the camera. The cat is sitting \n",
      "\n",
      "üñºÔ∏è Analyzing frame 9/16...\n",
      "üìù Frame 9 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat has a pink ribbon around its neck. The cat is sitting on a tiled floor. The cat is looking at the camera. The cat is wearing a pink ribbon around its neck. \n",
      "\n",
      "üñºÔ∏è Analyzing frame 10/16...\n",
      "üìù Frame 10 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, wearing a red collar. The cat is looking at the camera. The cat is wearing a red collar. The cat is sitting on the floor. The cat is \n",
      "\n",
      "üñºÔ∏è Analyzing frame 11/16...\n",
      "üìù Frame 11 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat has a pink bow on its head. The background is a bathroom with a mirror and a shower. The cat is sitting on the floor. The cat is looking at the camera. The cat is sitting on \n",
      "\n",
      "üñºÔ∏è Analyzing frame 12/16...\n",
      "üìù Frame 12 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking up at the camera. The cat is wearing a red bandana. The background is a room with a door and a mirror. The cat is sitting on the floor. The cat is looking up at the camera. The cat is sitting on the floor \n",
      "\n",
      "üñºÔ∏è Analyzing frame 13/16...\n",
      "üìù Frame 13 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat has a red collar on its neck. The cat is sitting on the floor. The cat is looking at the camera. The \n",
      "\n",
      "üñºÔ∏è Analyzing frame 14/16...\n",
      "üìù Frame 14 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat is wearing a red bandana. The background is a kitchen with a tiled floor. The cat is sitting on the floor. The cat is looking at the camera. The cat is looking \n",
      "\n",
      "üñºÔ∏è Analyzing frame 15/16...\n",
      "üìù Frame 15 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor. The cat is wearing a red bandana. The cat is looking at the camera. The cat is sitting \n",
      "\n",
      "üñºÔ∏è Analyzing frame 16/16...\n",
      "üìù Frame 16 cleaned result: Describe the main actions, objects, and sequence of events. The cat is sitting on the floor. The cat is wearing a red bandana. The cat is looking at the camera. The cat is looking \n",
      "\n",
      "‚úÖ Video analysis complete. 16 valid, cleaned analyses collected.\n",
      "[{'frame_index': 0, 'frame_path': 'sampled_frames\\\\frame_0.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat is wearing a red and white bandana. The cat is chewing on its paw. The cat is sitting on the floor. The cat is looking at the camera.'}, {'frame_index': 1, 'frame_path': 'sampled_frames\\\\frame_46.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat is wearing a red bandana. The cat is sitting on the floor. The cat is looking at the camera. The cat'}, {'frame_index': 2, 'frame_path': 'sampled_frames\\\\frame_93.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat is wearing a red collar. The background is a kitchen with a refrigerator. The cat is sitting on the floor. The cat is looking at the camera. The cat'}, {'frame_index': 3, 'frame_path': 'sampled_frames\\\\frame_140.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking up at the camera. The cat is wearing a red bow tie. The cat is sitting on the floor. The cat is looking up at the camera. The cat is looking'}, {'frame_index': 4, 'frame_path': 'sampled_frames\\\\frame_186.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, wearing a red bandana. The cat is looking at the camera. The cat is sitting on the floor. The cat is wearing a red bandana. The cat is sitting on'}, {'frame_index': 5, 'frame_path': 'sampled_frames\\\\frame_233.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor. The cat is wearing a red and white bow. The cat is looking at the camera. The cat is wearing a red and white'}, {'frame_index': 6, 'frame_path': 'sampled_frames\\\\frame_280.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is standing on its hind legs, looking up at the camera. The cat has a red collar on its neck. The background is a kitchen with a refrigerator and a stove. The cat is looking at the camera. The cat is not moving. The cat is not eating or drinking. The cat is not playing with the camera. The cat is not wearing a hat. The cat is not'}, {'frame_index': 7, 'frame_path': 'sampled_frames\\\\frame_327.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking up at the camera. The cat is wearing a red bow tie. The background is a room with a beige wall and a black door. The cat is sitting on the floor. The cat is looking up at the camera. The cat is sitting'}, {'frame_index': 8, 'frame_path': 'sampled_frames\\\\frame_373.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat has a pink ribbon around its neck. The cat is sitting on a tiled floor. The cat is looking at the camera. The cat is wearing a pink ribbon around its neck.'}, {'frame_index': 9, 'frame_path': 'sampled_frames\\\\frame_420.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, wearing a red collar. The cat is looking at the camera. The cat is wearing a red collar. The cat is sitting on the floor. The cat is'}, {'frame_index': 10, 'frame_path': 'sampled_frames\\\\frame_467.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat has a pink bow on its head. The background is a bathroom with a mirror and a shower. The cat is sitting on the floor. The cat is looking at the camera. The cat is sitting on'}, {'frame_index': 11, 'frame_path': 'sampled_frames\\\\frame_514.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking up at the camera. The cat is wearing a red bandana. The background is a room with a door and a mirror. The cat is sitting on the floor. The cat is looking up at the camera. The cat is sitting on the floor'}, {'frame_index': 12, 'frame_path': 'sampled_frames\\\\frame_560.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat has a red collar on its neck. The cat is sitting on the floor. The cat is looking at the camera. The'}, {'frame_index': 13, 'frame_path': 'sampled_frames\\\\frame_607.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor, looking at the camera. The cat is wearing a red bandana. The background is a kitchen with a tiled floor. The cat is sitting on the floor. The cat is looking at the camera. The cat is looking'}, {'frame_index': 14, 'frame_path': 'sampled_frames\\\\frame_654.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor. The cat is wearing a red bandana. The cat is looking at the camera. The cat is sitting'}, {'frame_index': 15, 'frame_path': 'sampled_frames\\\\frame_701.jpg', 'analysis': 'Describe the main actions, objects, and sequence of events. The cat is sitting on the floor. The cat is wearing a red bandana. The cat is looking at the camera. The cat is looking'}]\n"
     ]
    }
   ],
   "source": [
    "video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\What is MCP and how does it change AI_ (MCP explained) #ai #artificialintelligence.mp4\"\n",
    "\n",
    "video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\cat.mp4\"\n",
    "\n",
    "result = model.analyze_video(\n",
    "    video_path=video_path,\n",
    "    question=\"Describe the main actions, objects, and sequence of events.\",\n",
    "    num_frames=16\n",
    ")\n",
    "\n",
    "print(result[\"results\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4883a992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frames_analyzed': 4,\n",
       " 'results': [{'frame_index': 5,\n",
       "   'frame_path': 'sampled_frames\\\\frame_791.jpg',\n",
       "   'analysis': 'Describe the main actions, objects, and sequence of events. A diagram of a software development process is shown. The diagram shows a flowchart with the following steps: After MCP, LLM, MCP, and Model context. The diagram also includes icons for LLM, MCP, and Model context. The background is dark. The flowchart is divided into three main sections: LLM, MCP, and Model context. Each section has an icon and a text label. The LLM section has an icon of a laptop and a text label that reads \"LSTM\". The MCP section has an icon of a computer and'},\n",
       "  {'frame_index': 8,\n",
       "   'frame_path': 'sampled_frames\\\\frame_1266.jpg',\n",
       "   'analysis': 'Describe the main actions, objects, and sequence of events. A person is speaking into a microphone in a recording studio. The background is a studio with a microphone and a computer monitor. The person is wearing a black shirt. The scene is set in a recording studio. The person is speaking into a microphone, and the studio is filled with equipment. The person is speaking into a microphone, and the studio'},\n",
       "  {'frame_index': 11,\n",
       "   'frame_path': 'sampled_frames\\\\frame_1741.jpg',\n",
       "   'analysis': 'Describe the main actions, objects, and sequence of events. The main action is the person speaking in the microphone. The person is wearing a black shirt and has short brown hair. The background is a room with a microphone and a computer monitor. The scene is set in a recording studio. The person is speaking into the microphone, and the background shows a computer monitor and a microphone. The'},\n",
       "  {'frame_index': 14,\n",
       "   'frame_path': 'sampled_frames\\\\frame_2216.jpg',\n",
       "   'analysis': 'Describe the main actions, objects, and sequence of events. The man is wearing a black shirt and has a beard. He is sitting in a chair and speaking into a microphone. The background is a room with a couch and a lamp. The text \"from\" appears at the bottom of the screen. The scene is a close-up of the man speaking into the microphone. The man has a beard and is wearing a black shirt. The man has a beard and is'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06649b72",
   "metadata": {},
   "source": [
    "# 3. Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "384d4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from moviepy import VideoFileClip\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class SmolVLM2ChatModel:\n",
    "    \"\"\"\n",
    "    Extended SmolVLM2 wrapper that:\n",
    "      ‚úÖ Auto-selects model size based on GPU VRAM\n",
    "      ‚úÖ Supports bitsandbytes 4/8-bit quantization\n",
    "      ‚úÖ Uses new MoviePy API (.subclipped(), .with_volume_scaled())\n",
    "      ‚úÖ Trims videos into 20-second clips and analyzes each one\n",
    "    \"\"\"\n",
    "\n",
    "    MODEL_MAP = {\n",
    "        \"small\": \"HuggingFaceTB/SmolVLM2-256M-Instruct\",\n",
    "        \"medium\": \"HuggingFaceTB/SmolVLM2-500M-Instruct\",\n",
    "        \"large\": \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_size: Literal[\"small\", \"medium\", \"large\"] | None = None,\n",
    "        device: str | None = None,\n",
    "        dtype: torch.dtype = torch.bfloat16,\n",
    "        quantization: Literal[\"none\", \"8bit\", \"4bit\"] = \"none\",\n",
    "    ):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.quantization = quantization\n",
    "\n",
    "        # üîç Auto-select model size based on VRAM if not specified\n",
    "        if model_size is None:\n",
    "            model_size = self._auto_select_model_size()\n",
    "\n",
    "        self.model_name = self.MODEL_MAP.get(model_size, self.MODEL_MAP[\"medium\"])\n",
    "        print(f\"üîπ Loading {self.model_name} on {self.device} with {quantization} quantization...\")\n",
    "\n",
    "        # üß© Configure quantization (if enabled)\n",
    "        quant_config = None\n",
    "        if quantization in [\"8bit\", \"4bit\"]:\n",
    "            try:\n",
    "                quant_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=(quantization == \"8bit\"),\n",
    "                    load_in_4bit=(quantization == \"4bit\"),\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_compute_dtype=dtype,\n",
    "                )\n",
    "                print(f\"‚öôÔ∏è Using {quantization} quantization with bitsandbytes.\")\n",
    "            except Exception as e:\n",
    "                print(\"‚ö†Ô∏è Quantization not available, loading normally:\", e)\n",
    "                quant_config = None\n",
    "\n",
    "        # üß† Load processor and model\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\" if quant_config else None,\n",
    "            quantization_config=quant_config,\n",
    "        ).to(self.device if not quant_config else None)\n",
    "\n",
    "        print(f\"‚úÖ {model_size.upper()} SmolVLM2 model loaded successfully!\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _auto_select_model_size(self):\n",
    "        \"\"\"Automatically select model size based on available GPU VRAM.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"‚ö†Ô∏è CUDA not available. Using CPU-friendly 256M model.\")\n",
    "            return \"small\"\n",
    "\n",
    "        try:\n",
    "            total_vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            print(f\"üßÆ Detected VRAM: {total_vram_gb:.1f} GB\")\n",
    "\n",
    "            if total_vram_gb < 6:\n",
    "                return \"small\"\n",
    "            elif total_vram_gb < 10:\n",
    "                return \"medium\"\n",
    "            else:\n",
    "                return \"large\"\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Could not detect VRAM:\", e)\n",
    "            return \"medium\"\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _format_messages(self, messages):\n",
    "        if not isinstance(messages, list):\n",
    "            raise ValueError(\"messages must be a list of message dicts.\")\n",
    "        return messages\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _generate(self, formatted_messages, max_new_tokens=512, do_sample=False):\n",
    "        inputs = self.processor.apply_chat_template(\n",
    "            formatted_messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device, dtype=torch.bfloat16)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            do_sample=do_sample,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "        return self.processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def invoke(self, input):\n",
    "        formatted_messages = self._format_messages([input])\n",
    "        return {\"type\": \"ai_message\", \"content\": self._generate(formatted_messages)}\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def batch(self, inputs):\n",
    "        results = []\n",
    "        for inp in inputs:\n",
    "            formatted_messages = self._format_messages([inp])\n",
    "            response = self._generate(formatted_messages)\n",
    "            results.append({\"type\": \"ai_message\", \"content\": response})\n",
    "        return results\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def trim_video(self, video_path: str, clip_duration: int = 20):\n",
    "        \"\"\"\n",
    "        Trim a long video into multiple 20s clips using the new MoviePy API\n",
    "        and store them under `trimmed_videos/` beside the original video.\n",
    "        \"\"\"\n",
    "        video_path = Path(video_path)\n",
    "        output_dir = video_path.parent / \"trimmed_videos\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        print(f\"‚úÇÔ∏è Trimming video '{video_path.name}' into {clip_duration}s clips...\")\n",
    "\n",
    "        clips = []\n",
    "        with VideoFileClip(str(video_path)) as video:\n",
    "            total_duration = int(video.duration)\n",
    "\n",
    "            for i, start_time in enumerate(range(0, total_duration, clip_duration)):\n",
    "                end_time = min(start_time + clip_duration, total_duration)\n",
    "\n",
    "                # Trim and reduce volume (no text overlay)\n",
    "                trimmed = video.subclipped(start_time, end_time).with_volume_scaled(0.8)\n",
    "                output_path = output_dir / f\"{video_path.stem}_part{i+1}.mp4\"\n",
    "\n",
    "                trimmed.write_videofile(\n",
    "                    str(output_path),\n",
    "                    codec=\"libx264\",\n",
    "                    audio_codec=\"aac\",\n",
    "                )\n",
    "                clips.append(output_path)\n",
    "                print(f\"‚úÖ Saved: {output_path.name}\")\n",
    "\n",
    "        return clips\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    def analyze_video(self, video_path: str, user_query: str = \"Describe the main actions, objects, and sequence of events.\"):\n",
    "        \"\"\"\n",
    "        Trim a video into 20-second clips and analyze each sequentially.\n",
    "        \"\"\"\n",
    "        clips = self.trim_video(video_path, clip_duration=20)\n",
    "        results = []\n",
    "\n",
    "        print(f\"üîé Analyzing {len(clips)} clips...\")\n",
    "\n",
    "        for idx, clip_path in enumerate(clips, start=1):\n",
    "            print(clip_path)\n",
    "            print(f\"\\nüé• Analyzing Clip {idx}/{len(clips)}: {clip_path.name}\")\n",
    "\n",
    "            message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"video\", \"path\": video_path},\n",
    "                        {\"type\": \"text\", \"text\": user_query}\n",
    "                    ]\n",
    "            }\n",
    "\n",
    "            result = self.invoke(message)\n",
    "            results.append({\"clip\": clip_path.name, \"analysis\": result[\"content\"]})\n",
    "\n",
    "        return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ac941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading HuggingFaceTB/SmolVLM2-500M-Instruct on cuda with 4bit quantization...\n",
      "‚öôÔ∏è Using 4bit quantization with bitsandbytes.\n",
      "‚úÖ MEDIUM SmolVLM2 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = SmolVLM2ChatModel(model_size=\"medium\", quantization=\"4bit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "084af228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Trimming video 'What is MCP and how does it change AI_ (MCP explained) #ai #artificialintelligence_part1.mp4' into 20s clips...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.05 MiB for an array with shape (400000,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\cat.mp4\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat did you see in the video?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39manalyze_video(video_path, user_query\u001b[38;5;241m=\u001b[39muser_query)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müé¨ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 164\u001b[0m, in \u001b[0;36mSmolVLM2ChatModel.analyze_video\u001b[1;34m(self, video_path, user_query)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_video\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_path: \u001b[38;5;28mstr\u001b[39m, user_query: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe the main actions, objects, and sequence of events.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    Trim a video into 20-second clips and analyze each sequentially.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     clips \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrim_video(video_path, clip_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    165\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîé Analyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(clips)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m clips...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 139\u001b[0m, in \u001b[0;36mSmolVLM2ChatModel.trim_video\u001b[1;34m(self, video_path, clip_duration)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÇÔ∏è Trimming video \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclip_duration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms clips...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m clips \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m VideoFileClip(\u001b[38;5;28mstr\u001b[39m(video_path)) \u001b[38;5;28;01mas\u001b[39;00m video:\n\u001b[0;32m    140\u001b[0m     total_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(video\u001b[38;5;241m.\u001b[39mduration)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, start_time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, total_duration, clip_duration)):\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\moviepy\\decorators.py:102\u001b[0m, in \u001b[0;36mpreprocess_args.<locals>.decor.<locals>.wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m new_args \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     91\u001b[0m     (\n\u001b[0;32m     92\u001b[0m         preprocess_func(arg)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, argnames)\n\u001b[0;32m     97\u001b[0m ]\n\u001b[0;32m     98\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     99\u001b[0m     kwarg: preprocess_func(value) \u001b[38;5;28;01mif\u001b[39;00m kwarg \u001b[38;5;129;01min\u001b[39;00m varnames \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (kwarg, value) \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    101\u001b[0m }\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39mnew_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\moviepy\\video\\io\\VideoFileClip.py:144\u001b[0m, in \u001b[0;36mVideoFileClip.__init__\u001b[1;34m(self, filename, decode_file, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, fps_source, pixel_format, is_mask)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Make a reader for the audio, if any.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39minfos[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_found\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio \u001b[38;5;241m=\u001b[39m AudioFileClip(\n\u001b[0;32m    145\u001b[0m         filename,\n\u001b[0;32m    146\u001b[0m         buffersize\u001b[38;5;241m=\u001b[39maudio_buffersize,\n\u001b[0;32m    147\u001b[0m         fps\u001b[38;5;241m=\u001b[39maudio_fps,\n\u001b[0;32m    148\u001b[0m         nbytes\u001b[38;5;241m=\u001b[39maudio_nbytes,\n\u001b[0;32m    149\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\moviepy\\decorators.py:102\u001b[0m, in \u001b[0;36mpreprocess_args.<locals>.decor.<locals>.wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m new_args \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     91\u001b[0m     (\n\u001b[0;32m     92\u001b[0m         preprocess_func(arg)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, argnames)\n\u001b[0;32m     97\u001b[0m ]\n\u001b[0;32m     98\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     99\u001b[0m     kwarg: preprocess_func(value) \u001b[38;5;28;01mif\u001b[39;00m kwarg \u001b[38;5;129;01min\u001b[39;00m varnames \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (kwarg, value) \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    101\u001b[0m }\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39mnew_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\moviepy\\audio\\io\\AudioFileClip.py:65\u001b[0m, in \u001b[0;36mAudioFileClip.__init__\u001b[1;34m(self, filename, decode_file, buffersize, nbytes, fps)\u001b[0m\n\u001b[0;32m     62\u001b[0m AudioClip\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader \u001b[38;5;241m=\u001b[39m FFMPEG_AudioReader(\n\u001b[0;32m     66\u001b[0m     filename,\n\u001b[0;32m     67\u001b[0m     decode_file\u001b[38;5;241m=\u001b[39mdecode_file,\n\u001b[0;32m     68\u001b[0m     fps\u001b[38;5;241m=\u001b[39mfps,\n\u001b[0;32m     69\u001b[0m     nbytes\u001b[38;5;241m=\u001b[39mnbytes,\n\u001b[0;32m     70\u001b[0m     buffersize\u001b[38;5;241m=\u001b[39mbuffersize,\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfps \u001b[38;5;241m=\u001b[39m fps\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mduration\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\moviepy\\audio\\io\\readers.py:71\u001b[0m, in \u001b[0;36mFFMPEG_AudioReader.__init__\u001b[1;34m(self, filename, buffersize, decode_file, print_infos, fps, nbytes, nchannels)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_startframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_around(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\moviepy\\audio\\io\\readers.py:288\u001b[0m, in \u001b[0;36mFFMPEG_AudioReader.buffer_around\u001b[1;34m(self, frame_number)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(new_bufferstart)\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffersize)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_startframe \u001b[38;5;241m=\u001b[39m new_bufferstart\n",
      "File \u001b[1;32mc:\\Users\\jthxc\\anaconda3\\envs\\jaredllm\\Lib\\site-packages\\moviepy\\audio\\io\\readers.py:161\u001b[0m, in \u001b[0;36mFFMPEG_AudioReader.read_chunk\u001b[1;34m(self, chunksize)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfromstring(s, dtype\u001b[38;5;241m=\u001b[39mdata_type)\n\u001b[1;32m--> 161\u001b[0m result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m result \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbytes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m    162\u001b[0m     (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnchannels), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnchannels)\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Pad the read chunk with zeros when there isn't enough audio\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# left to read, so the buffer is always at full length.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m pad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((chunksize \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(result), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnchannels), dtype\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.05 MiB for an array with shape (400000,) and data type float64"
     ]
    }
   ],
   "source": [
    "video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\What is MCP and how does it change AI_ (MCP explained) #ai #artificialintelligence.mp4\"\n",
    "\n",
    "video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\trimmed_videos\\What is MCP and how does it change AI_ (MCP explained) #ai #artificialintelligence_part1.mp4\"\n",
    "#video_path = r\"C:\\Interview Assignment\\Intel GenAI Software Engineer Assessment\\Short-Video-Analyst\\sample_videos\\cat.mp4\"\n",
    "user_query = \"What did you see in the video?\"\n",
    "\n",
    "results = model.analyze_video(video_path, user_query=user_query)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"\\nüé¨ {r['clip']} ‚Üí {r['analysis']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf2426d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clip': 'cat_part1.mp4',\n",
       "  'analysis': 'User: Describe the main actions, objects, and sequence of events.<video>\\nAssistant: The video shows a person standing in front of a large, modern building with a glass facade. The person is wearing a black jacket and a black hat, and they are holding a black bag. The building has a large glass window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding'},\n",
       " {'clip': 'cat_part2.mp4',\n",
       "  'analysis': 'User: Describe the main actions, objects, and sequence of events.<video>\\nAssistant: The video shows a person standing in front of a large, modern building with a glass facade. The person is wearing a black jacket and a black hat, and they are holding a black bag. The building has a large glass window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding a black bag. The room has a large window on the side, and the person appears to be looking out at the view.\\n\\nThe video then cuts to a close-up of a person sitting at a desk in a room with a large window. The person is wearing a black shirt and a black hat, and they are holding'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416ecdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaredllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
